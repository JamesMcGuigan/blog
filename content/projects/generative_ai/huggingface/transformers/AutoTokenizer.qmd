---
title: "AutoTokenizer"
author: GPT4
source: https://chat.openai.com/c/6c316576-a166-4a3b-bcfc-cb2935c4abc1
---

## Basic Functions

The `AutoTokenizer` class in the Hugging Face `transformers` library is a versatile tool designed to handle tokenization tasks for a wide range of pre-trained models. It abstracts away the specifics of each tokenizer, allowing you to work with various models without worrying about the underlying tokenizer details. Here's a rundown of some basic functions and how they're typically used:

### 1. from_pretrained()

The most common way to instantiate a tokenizer. This method automatically fetches and loads the tokenizer associated with a given pre-trained model.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
```

### 2. encode() and encode_plus()

These methods convert text into token IDs. `encode()` returns a list of token IDs, while `encode_plus()` provides additional outputs like attention masks, token type IDs, and more, typically required by models for proper input formatting.

```python
# Simple encoding
token_ids = tokenizer.encode("Hello, world!")

# Advanced encoding with additional features
encoded = tokenizer.encode_plus(
    "Hello, world!",
    add_special_tokens=True,  # Adds special tokens (like [CLS], [SEP])
    return_attention_mask=True,  # Returns attention mask
    pad_to_max_length=True,  # Pads to max sequence length
    max_length=512,  # Specifies max sequence length
    return_tensors='pt',  # Returns PyTorch tensors
)
```

### 3. decode()

This function converts token IDs back to readable text. It's particularly useful for interpreting the outputs of your model.

```python
decoded_text = tokenizer.decode(token_ids)
```

### 4. batch_encode_plus() and __call__()

For processing multiple texts at once (like sentences or documents), these methods are very efficient. They return a batch of encoded inputs with consistent length, which is essential for feeding batches of data into your model.

```python
batch_encoded = tokenizer.batch_encode_plus(
    ["Hello, world!", "Another text."],
    padding=True,  # Adds padding to ensure consistent length
    return_tensors='pt'  # Returns PyTorch tensors
)

# Or using __call__(), which is equivalent to using batch_encode_plus()
batch_encoded = tokenizer(
    ["Hello, world!", "Another text."],
    padding=True,
    return_tensors='pt'
)
```

### 5. convert_tokens_to_ids() and convert_ids_to_tokens()

These methods are for converting between tokens and their corresponding IDs, allowing for more granular manipulation of the tokenization process.

```python
tokens = ['[CLS]', 'hello', ',', 'world', '!', '[SEP]']
token_ids = tokenizer.convert_tokens_to_ids(tokens)

# Convert back to tokens
tokens_back = tokenizer.convert_ids_to_tokens(token_ids)
```

### Usage Tips:

- When using these functions, especially for training or evaluation, make sure to understand the expected input format of your model. For instance, some models require special tokens to be added to the inputs, which `encode_plus()` and `__call__()` can handle with the `add_special_tokens=True` argument.
- The `padding` and `truncation` arguments are crucial for batch processing to ensure all inputs are of the same length. This is important for models that expect fixed-size inputs.
- The `return_tensors` argument allows you to specify the format of the returned tensors (`'pt'` for PyTorch, `'tf'` for TensorFlow, and `'np'` for NumPy arrays), ensuring compatibility with your training or inference pipeline.

These basic functions of `AutoTokenizer` provide a strong foundation for preprocessing text data, making it easier to work with various NLP models. If you have any specific use cases or advanced scenarios in mind, feel free to dive deeper or ask for more detailed examples!


## Worked Example

Let's work through a detailed example using `AutoTokenizer` from the Hugging Face `transformers` library. We'll use the `bert-base-uncased` model as our base for this example, focusing on tokenization, encoding, and decoding processes.

### Setting Up

First, ensure you have the `transformers` library installed. If not, you can install it using pip:

```sh
pip install transformers
```

## Step 1: Initializing the Tokenizer

We'll start by importing `AutoTokenizer` and initializing it with the `bert-base-uncased` pre-trained model.

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
```

### Step 2: Tokenizing Text

Let's tokenize a simple sentence: "Hello, world! This is a test."

```python
text = "Hello, world! This is a test."

# Tokenize the text
tokens = tokenizer.tokenize(text)
print(tokens)
```

**Output:**

```
['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']
```

Here, the text is lowercased (since we're using the `uncased` version of BERT), and punctuation is separated as individual tokens.

### Step 3: Encoding

Now, we'll convert our text into token IDs using `encode_plus`, which provides additional outputs necessary for model inputs.

```python
encoded_inputs = tokenizer.encode_plus(
    text,
    add_special_tokens=True,  # Adds [CLS] and [SEP]
    return_attention_mask=True,  # Generates attention mask
    padding='max_length',  # Pads to a length
    max_length=20,  # Specifies maximum length
    return_tensors='pt',  # Returns PyTorch tensors
)

print(encoded_inputs)
```

**Output:**

This will produce a dictionary containing the following keys: `input_ids`, `token_type_ids`, and `attention_mask`, all of which are PyTorch tensors due to `return_tensors='pt'`. The `input_ids` are the token IDs, `token_type_ids` are used for models that require a distinction between sentence pairs (not necessary here), and `attention_mask` indicates to the model which tokens should be attended to, and which should not (e.g., padding tokens).

### Step 4: Decoding

Finally, let's decode the token IDs back to text to see how the tokenizer converts IDs back into a string. We'll decode the `input_ids` from our encoded inputs.

```python
decoded_text = tokenizer.decode(encoded_inputs['input_ids'][0], skip_special_tokens=True)
print(decoded_text)
```

**Output:**

```
hello, world! this is a test.
```

The decoded text is a clean version of the original text, omitting special tokens that were added during the encoding process.

### Complete Python Script

Here's the entire process in one Python script:

```python
from transformers import AutoTokenizer

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# The text to tokenize, encode, and decode
text = "Hello, world! This is a test."

# Tokenize the text
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

# Encode the text
encoded_inputs = tokenizer.encode_plus(
    text,
    add_special_tokens=True,
    return_attention_mask=True,
    padding='max_length',
    max_length=20,
    return_tensors='pt',
)
print("Encoded Inputs:", encoded_inputs)

# Decode the token IDs back to text
decoded_text = tokenizer.decode(encoded_inputs['input_ids'][0], skip_special_tokens=True)
print("Decoded Text:", decoded_text)
```

This script demonstrates the basic functionality of the `AutoTokenizer`: tokenizing a piece of text, encoding it into a format suitable for a model, and then decoding the output back into human-readable text. This process is fundamental for preparing text data for training or inference with NLP models.

## Models

The Hugging Face `transformers` library supports a wide range of tokenizer models, each designed to work with specific types of pre-trained language models. These tokenizers can handle different tokenization strategies, such as Byte-Pair Encoding (BPE), SentencePiece, WordPiece, and more. Below is a list of some of the tokenizer model families and the pre-trained models they are associated with:

### 1. BERT Tokenizers

- **Model Family:** BERT (Bidirectional Encoder Representations from Transformers)
- **Example Models:** `bert-base-uncased`, `bert-large-cased`, etc.
- **Tokenizer:** WordPiece

### 2. GPT and GPT-2 Tokenizers

- **Model Family:** GPT (Generative Pre-trained Transformer), GPT-2
- **Example Models:** `openai-gpt`, `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`
- **Tokenizer:** Byte-Pair Encoding (BPE)

### 3. RoBERTa Tokenizers

- **Model Family:** RoBERTa (Robustly Optimized BERT Approach)
- **Example Models:** `roberta-base`, `roberta-large`, `roberta-large-mnli`
- **Tokenizer:** Byte-Level BPE

### 4. DistilBERT Tokenizers

- **Model Family:** DistilBERT (a distilled version of BERT)
- **Example Models:** `distilbert-base-uncased`, `distilbert-base-cased`
- **Tokenizer:** WordPiece

### 5. XLNet Tokenizers

- **Model Family:** XLNet
- **Example Models:** `xlnet-base-cased`, `xlnet-large-cased`
- **Tokenizer:** SentencePiece

### 6. T5 Tokenizers

- **Model Family:** T5 (Text-to-Text Transfer Transformer)
- **Example Models:** `t5-small`, `t5-base`, `t5-large`, `t5-3b`, `t5-11b`
- **Tokenizer:** SentencePiece

### 7. ALBERT Tokenizers

- **Model Family:** ALBERT (A Lite BERT)
- **Example Models:** `albert-base-v2`, `albert-large-v2`, etc.
- **Tokenizer:** SentencePiece

### 8. BART Tokenizers

- **Model Family:** BART (Bidirectional and Auto-Regressive Transformers)
- **Example Models:** `facebook/bart-base`, `facebook/bart-large`
- **Tokenizer:** Byte-Level BPE

### 9. ELECTRA Tokenizers

- **Model Family:** ELECTRA
- **Example Models:** `google/electra-small-generator`, `google/electra-base-discriminator`
- **Tokenizer:** WordPiece

### 10. DeBERTa Tokenizers

- **Model Family:** DeBERTa (Decoding-enhanced BERT with Disentangled Attention)
- **Example Models:** `microsoft/deberta-base`, `microsoft/deberta-large`
- **Tokenizer:** SentencePiece

### Specialized Tokenizers

- **CTRL (Salesforce):** Uses a special tokenizer with additional control codes.
- **Longformer:** Designed for longer documents by providing an extended positional encoding.
- **Reformer:** Optimized for efficiency with reversible layers and locality-sensitive hashing.

Each tokenizer is tailored to its respective model architecture, ensuring that text inputs are correctly preprocessed for training or inference tasks. When using `AutoTokenizer` with the `from_pretrained` method, the correct tokenizer for your chosen pre-trained model is automatically instantiated, abstracting away the need to manually select the appropriate tokenizer class.